= Transfer tools
:toc:
:icons:
:linkattrs:
:imagesdir: ../resources/images


== Summary

This section will compare and demonstrate how different file transfer tools affect performance when accessing an EFS file system.


== Duration

NOTE: It will take approximately 15 minutes to complete this section.


== Step-by-step Guide

=== Transfer tools

IMPORTANT: Read through all steps below and watch the quick video before continuing.

image::transfer-tools.gif[align="left", width=600]

. Return to the browser-based SSH connection of the *EFS Workshop Linux Instance 2* instance.
+
TIP: If the SSH connection has timed out, e.g. the session is unresponsive, close the browser-based SSH connection window and create a new one. Return to the link:https://console.aws.amazon.com/ec2/[Amazon EC2] console. *_Click_* the radio button next to the instance with the name *EFS Workshop Linux Instance 2*. *_Click_* the *Connect* button. *_Click_* the radio button next to  *EC2 Instance Connect (browser-based SSH connection)*.Leave the default user name as *ec2-user* and *_click_* *Connect*.
+
. This instance was reloaded with approx. five thousand 1 MiB files totaling approx. 5 GB of data, all stored on an the attached EBS GP2 volume. This section will use different transfer tools to copy this dataset from EBS to EFS.
. Run this command to validate the total size and count of files to be copied.
[source,bash]
----
tree --du -h /ebs/data-1m

----
* The ouput should look similar to this:

[source,bash]
----
    ├── [1.0M]  _ip-10-0-0-11_00_994_
    ├── [1.0M]  _ip-10-0-0-11_00_995_
    ├── [1.0M]  _ip-10-0-0-11_00_996_
    ├── [1.0M]  _ip-10-0-0-11_00_997_
    ├── [1.0M]  _ip-10-0-0-11_00_998_
    └── [1.0M]  _ip-10-0-0-11_00_999_

 4.9G used in 1 directory, 5000 files
----


=== Rsync

rsync is a fast, versatile, remote (and local) file-copying tool. Learn more about rsync - link:https://linux.die.net/man/1/rsync[https://linux.die.net/man/1/rsync].

. *_Run_* the following rsync command in the browser-based SSH connection window to see how long it takes to copy the sample dataset from EBS to EFS.
+
[source,bash]
----
instance_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
time rsync -r /ebs/data-1m/ /efs/rsync/${instance_id}

----
+
* How long did it take for the copy?
* The ouput of the script should look similar to this:
+
[source,bash]
----
real	5m59.205s
user	0m19.346s
sys	    0m6.019s
----
+
* Calculate the average throughput achieved. Divide 5000 MB by the number of seconds it took for the copy, e.g. 5000(MB)÷359(seconds)=13.93MB/s
* Why is rsync so slow?
  * rync is a single-threaded copy tool that is very chatty over the network. These two attributes don't make rsync a good tool to use to copy data to and from an EFS file system because it doesn't take advantage of the distributed data storage design of EFS.


=== Copy (cp)

. *_Run_* the following cp command to see how long it takes to copy the sample dataset from EBS to EFS.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
time cp -r /ebs/data-1m/* /efs/cp/${instance_id}

----
+
* How long did it take for the copy?
* The ouput of the script should look similar to this:
+
[source,bash]
----
real	4m34.786s
user	0m0.048s
sys	    0m4.584s
----
+
* Calculate the average throughput achieved. Divide 5000 MB by the number of seconds it took for the copy, e.g. 5000(MB)÷274(seconds)=18.25MB/s
* Why is so slow but faster than rsync?
  * cp is also a single-threaded copy tool but isn't as chatty over the network as rsync, to throughput is faster.


=== fpsync

fpsync is a tool included in fpart and is a powerful shell script that wraps fpart and rsync to launch multiple transfer jobs in parallel. Learn more about fpsync - link:https://github.com/martymac/fpart#fpsync-[https://github.com/martymac/fpart#fpsync-].

Fpart has been written by Ganael LAPLANCHE ganael.laplanche@martymac.org and is available under the BSD license (see COPYING for details).

. *_Run_* the following fpsync command to see how long it takes to copy the sample dataset from EBS to EFS.
* The first command sets the $threads variable to 4 threads per virtual cpu (vcpu). This will be used by the multi-threaded transfer tools.
+
[source,bash]
----
threads=$(($(nproc --all) * 4))
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
time fpsync -n ${threads} -v /ebs/data-1m/ /efs/fpsync/${instance_id}

----
+
* How long did it take for the copy?
* The ouput of the script should look similar to this:
+
[source,bash]
----
real	4m34.786s
user	0m0.048s
sys	    0m4.584s
----
+
* Calculate the average throughput achieved. Divide 5000 MB by the number of seconds it took for the copy, e.g. 5000(MB)÷274(seconds)=18.25MB/s
* Why is so slow but faster than rsync?
  * cp is also a single-threaded copy tool but isn't as chatty over the network as rsync, to throughput is faster.














. What was the IOPS?
. How many threads were used?
. Were the files generated in the same directory?
* HINT: Look at the value of the variable "--same-dir".
. *_Copy_* the previous smallfile script to your favorite text editor. Experiment with different smallfile parameter settings. Use the table below as a guide. Test with different threads (--threads), file size (--file-size), file count (--file-count) and same directory (--same-dir).
+
[cols="10,5"]
|===
| Parameter | Description

| `--threads`
a| Number of threads.

| `--file-size`
a| File size in KB.

| `--file-count`
a| Number of files per thread. For example, if you want to see how long it takes to generate 1024 files using 16 threads, change the --threads parameter to 16 and the --file-count parameter to 64 (1024÷16=64).

| `--same-dir`
a| Y will generate all files in the same direct - increasing inode contention. N will generate files in different directories, one for each thread - decreasing inode contention.

|===
+

* What different parameters did you test?
* How did the different parameter options alter the results?
* The following table and graphs show the sample results of a few tests. Look how increasing the number of threads (increasing parallelism) and writing to different subdirectories (decreasing inode contention) impacts the IOPS and duration.

+
[cols="3,3,2,3,3,3,3",options="header"]
|===
|Threads |File size (KB) |File count (per thread) |File count (total) |Same directory |Duration (seconds) |IOPS

| 1
| 4
| 1024
| 1024
| Y
| 11.369
| 90.066095

| 2
| 4
| 512
| 1024
| Y
| 5.820
| 176.009550

| 4
| 4
| 256
| 1024
| Y
| 5.883
| 174.591562

| 8
| 4
| 128
| 1024
| Y
| 5.882
| 175.117492

| 16
| 4
| 64
| 1024
| Y
| 5.629
| 184.055531

| 32
| 4
| 32
| 1024
| Y
| 5.641
| 186.835993

| 1
| 4
| 1024
| 1024
| N
| 11.958
| 85.633895

| 2
| 4
| 512
| 1024
| N
| 5.452
| 188.621103

| 4
| 4
| 256
| 1024
| N
| 2.755
| 372.936600

| 8
| 4
| 128
| 1024
| N
| 1.390
| 746.051127

| 16
| 4
| 64
| 1024
| N
| 0.819
| 1281.790673

| 32
| 4
| 32
| 1024
| N
| 0.535
| 1973.441341

|===

--
[.left]
.IOPS
image:iops-4kb-iops-graph.png[450, scaledwidth="75%"]
[.left]
.Duration
image:iops-4kb-duration-graph.png[450, scaledwidth="75%"]
--

== Next section

Click the link below to go to the next section.

image::iops-4k.png[link=../04-iops-4k/, align="left",width=420]




