= Parallelism and throughput using IOR
:toc:
:icons:
:linkattrs:
:imagesdir: ../resources/images


== Summary

This section will demonstrate how increasing the number of threads accessing an EFS file system will significantly improve throughput.

== Duration

NOTE: It will take approximately 15 minutes to complete this section.


== Step-by-step Guide

=== Different levels of parallelism

IMPORTANT: Read through all steps below and watch the quick video before continuing.

image::throughput-ior.gif[align="left", width=600]

. Return to the browser-based SSH connection of the *EFS Workshop Linux Instance 2* - m5n.2xlarge instance.
+
TIP: If the SSH connection has timed out, e.g. the session is unresponsive, close the browser-based SSH connection window and create a new one. Return to the link:https://console.aws.amazon.com/ec2/[Amazon EC2] console. *_Click_* the radio button next to the instance with the name *EFS Workshop Linux Instance 0*. *_Click_* the *Connect* button. *_Click_* the radio button next to  *EC2 Instance Connect (browser-based SSH connection)*.Leave the default user name as *ec2-user* and *_click_* *Connect*.
+
. *_Run_* the following IOR command to generate one 2 GiB file using one thread on an EFS file system. IOR is a commonly used file system benchmarking application typically used to evaluate the performance of distributed and parallel file systems. You can read more about IOR here - link:http://wiki.lustre.org/IOR[http://wiki.lustre.org/IOR].
+
[source,bash]
----
module load mpi/openmpi-x86_64
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 1 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 2048 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* The results should look similar to this.
+
[source,bash]
----
IOR-3.3.0+dev: MPI Coordinated Test of Parallel I/O
Began               : Mon Jun  1 19:21:30 2020
Command line        : ior --posix.odirect -t 1m -b 1m -s 2048 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin
Machine             : Linux ip-10-0-0-11
Start time skew across all tasks: 0.00 sec
TestID              : 0
StartTime           : Mon Jun  1 19:21:30 2020
Path                : /efs/ior
FS                  : 8388608.0 TiB   Used FS: 0.0%   Inodes: 0.0 Mi   Used Inodes: -nan%
Participating tasks: 1

Options:
api                 : POSIX
apiVersion          :
test filename       : /efs/ior/ior.bin
access              : single-shared-file
type                : independent
segments            : 2048
ordering in a file  : sequential
ordering inter file : no tasks offsets
nodes               : 1
tasks               : 1
clients per node    : 1
repetitions         : 1
xfersize            : 1 MiB
blocksize           : 1 MiB
aggregate filesize  : 2 GiB

Results:

access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----
Commencing write performance test: Mon Jun  1 19:21:30 2020
write     24.38      24.39      83.98       1024.00    1024.00    0.004828   83.98      0.000897   83.99      0
Max Write: 24.38 MiB/sec (25.57 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev   Max(OPs)   Min(OPs)  Mean(OPs)     StdDev    Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt   blksiz    xsize aggs(MiB)   API RefNum
write          24.38      24.38      24.38       0.00      24.38      24.38      24.38       0.00   83.99048         NA            NA     0      1   1    1   0     0        1         0    0   2048  1048576  1048576    2048.0 POSIX      0
Finished            : Mon Jun  1 19:22:54 2020
----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate one 2 GiB file using two threads on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 2 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 1024 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate one 2 GiB file using four threads on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 4 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 512 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate one 2 GiB file using eight threads on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 8 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 256 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate one 2 GiB file using sixteen threads on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 16 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 128 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate one 2 GiB file using thirty-two threads on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 32 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 64 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate one 2 GiB file using sixty-four threads on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 64 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 32 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread using one thread (e.g. one file one directory) on an EFS file system (notice the new *-F* filePerProc -- file-per-process flag).
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 1 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 2048 -g -v -w -i 1 -F -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread using two threads (e.g. two files one directory) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 2 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 1024 -g -v -w -i 1 -F -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread using four threads (e.g. four files one directory) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 4 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 512 -g -v -w -i 1 -F -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread using eight threads (e.g. eight files one directory) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 8 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 256 -g -v -w -i 1 -F -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread using sixteen threads (e.g. sixteen files one directory) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 16 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 128 -g -v -w -i 1 -F -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread using thirty-two threads (e.g. thirty-two files one directory) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 32 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 64 -g -v -w -i 1 -F -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread using sixty-four threads (e.g. sixty-four files one directory) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 64 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 32 -g -v -w -i 1 -F -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread per directory using one thread (e.g. one file one directory) on an EFS file system (notice the new flag *-u* uniqueDir -- use unique directory name for each file-per-process; also added *-k* keepFile -- don't remove the test file(s) on program exit -- to keep the files for the next round of read tests).
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 1 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 2048 -g -v -w -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread per directory using two threads (e.g. two files two directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 2 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 1024 -g -v -w -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread per directory using four threads (e.g. four files four directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 4 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 512 -g -v -w -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread per directory using eight threads (e.g. eight files eight directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 8 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 256 -g -v -w -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread per directory using sixteen threads (e.g. sixteen files sixteen directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 16 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 128 -g -v -w -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread per directory using thirty-two threads (e.g. thirty-two files thirty-two directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 32 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 64 -g -v -w -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate 2 GiBs of data with one file per thread per directory using sixty-four threads (e.g. sixty-four files sixty-four directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 64 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 32 -g -v -w -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to read 2 GiBs of data from the previous write test with one file per thread per directory using one thread (e.g. one file one directory) on an EFS file system (notice the replacement of *-w* writeFile -- write file with *-r* readFile -- read existing file; also removed *-k* keepFile -- don't remove the test file(s) on program exit -- to clean up the test files).
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 1 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 2048 -g -v -r -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to read 2 GiBs of data from the previous write test with one file per thread per directory using two threads (e.g. two files two directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 2 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 1024 -g -v -r -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to read 2 GiBs of data from the previous write test with one file per thread per directory using four threads (e.g. four files four directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 4 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 512 -g -v -r -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to read 2 GiBs of data from the previous write test with one file per thread per directory using eight threads (e.g. eight files eight directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 8 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 256 -g -v -r -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to read 2 GiBs of data from the previous write test with one file per thread per directory using sixteen threads (e.g. sixteen files sixteen directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 16 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 128 -g -v -r -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to read 2 GiBs of data from the previous write test with one file per thread per directory using thirty-two threads (e.g. thirty-two files thirty-two directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 32 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 64 -g -v -r -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to read 2 GiBs of data from the previous write test with one file per thread per directory using sixty-four threads (e.g. sixty-four files sixty-four directories) on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 64 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 32 -g -v -r -i 1 -u -F -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?

. Compare the results from the tests above.  Is there a big difference? Why?

* The following table and graphs show the sample results of these tests. Look how increasing the size of the IO (reducing sync operations) and increasing the number of threads (increasing parallelism) impacts the throughput and duration.

+

|==============================================================================================
| Operation | Threads | File count | Directory per file | Duration (seconds) | Throughput (MB/s)
| Write     | 1       | 1          | N                  | 84.74              | 25.34
| Write     | 1       | 2          | N                  | 61.48              | 34.93
| Write     | 1       | 4          | N                  | 24.46              | 87.78
| Write     | 1       | 8          | N                  | 14.24              | 150.79
| Write     | 1       | 16         | N                  | 10.83              | 198.36
| Write     | 1       | 32         | N                  | 10.31              | 208.32
| Write     | 1       | 64         | N                  | 9.68               | 221.82
| Write     | 1       | 1          | N                  | 85.36              | 25.16
| Write     | 2       | 2          | N                  | 43.45              | 49.43
| Write     | 4       | 4          | N                  | 22.27              | 96.44
| Write     | 8       | 8          | N                  | 11.80              | 181.93
| Write     | 16      | 16         | N                  | 8.93               | 240.42
| Write     | 32      | 32         | N                  | 7.87               | 272.83
| Write     | 64      | 64         | N                  | 8.06               | 266.46
| Write     | 1       | 1          | Y                  | 84.69              | 25.36
| Write     | 2       | 2          | Y                  | 42.65              | 50.35
| Write     | 4       | 4          | Y                  | 22.05              | 97.37
| Write     | 8       | 8          | Y                  | 12.24              | 175.41
| Write     | 16      | 16         | Y                  | 8.16               | 263.02
| Write     | 32      | 32         | Y                  | 7.69               | 279.16
| Write     | 64      | 64         | Y                  | 7.64               | 281.12
| Read      | 1       | 1          | Y                  | 34.63              | 62.01
| Read      | 2       | 2          | Y                  | 17.03              | 126.09
| Read      | 4       | 4          | Y                  | 8.95               | 239.82
| Read      | 8       | 8          | Y                  | 5.13               | 418.44
| Read      | 16      | 16         | Y                  | 5.16               | 415.94
| Read      | 32      | 32         | Y                  | 5.16               | 415.96
| Read      | 64      | 64         | Y                  | 5.21               | 412.06

|==============================================================================================

--
[.left]
.IOPS
image::throughput-dd-throughput-graph.png[450, scaledwidth="75%"]
[.left]
.Duration
image::throughput-dd-duration-graph.png[450, scaledwidth="75%"]
--


== Next section

Click the link below to go to the next section.

image::transfer-tools.png[link=../09-transfer-tools, align="left",width=420]

