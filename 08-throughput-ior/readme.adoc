= Parallelism and throughput using IOR
:toc:
:icons:
:linkattrs:
:imagesdir: ../resources/images


== Summary

This section will demonstrate how increasing the number of threads accessing an EFS file system will significantly improve throughput.

== Duration

NOTE: It will take approximately 15 minutes to complete this section.


== Step-by-step Guide

=== Different levels of parallelism

IMPORTANT: Read through all steps below and watch the quick video before continuing.

image::throughput-ior.gif[align="left", width=600]

. Return to the browser-based SSH connection of the *EFS Workshop Linux Instance 2* - m5n.2xlarge instance.
+
TIP: If the SSH connection has timed out, e.g. the session is unresponsive, close the browser-based SSH connection window and create a new one. Return to the link:https://console.aws.amazon.com/ec2/[Amazon EC2] console. *_Click_* the radio button next to the instance with the name *EFS Workshop Linux Instance 0*. *_Click_* the *Connect* button. *_Click_* the radio button next to  *EC2 Instance Connect (browser-based SSH connection)*.Leave the default user name as *ec2-user* and *_click_* *Connect*.
+
. *_Run_* the following IOR command to generate one 2 GiB file using one thread on an EFS file system. IOR is a commonly used file system benchmarking application typically used to evaluate the performance of distributed and parallel file systems. You can read more about IOR here - link:http://wiki.lustre.org/IOR[http://wiki.lustre.org/IOR].
+
[source,bash]
----
module load mpi/openmpi-x86_64
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 1 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 2048 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* The results should look similar to this.
[source,bash]
----
IOR-3.3.0+dev: MPI Coordinated Test of Parallel I/O
Began               : Mon Jun  1 19:21:30 2020
Command line        : ior --posix.odirect -t 1m -b 1m -s 2048 -g -v -w -i 1 -k -D 0 -o /efs/ior/ior.bin
Machine             : Linux ip-10-0-0-11
Start time skew across all tasks: 0.00 sec
TestID              : 0
StartTime           : Mon Jun  1 19:21:30 2020
Path                : /efs/ior
FS                  : 8388608.0 TiB   Used FS: 0.0%   Inodes: 0.0 Mi   Used Inodes: -nan%
Participating tasks: 1

Options:
api                 : POSIX
apiVersion          :
test filename       : /efs/ior/ior.bin
access              : single-shared-file
type                : independent
segments            : 2048
ordering in a file  : sequential
ordering inter file : no tasks offsets
nodes               : 1
tasks               : 1
clients per node    : 1
repetitions         : 1
xfersize            : 1 MiB
blocksize           : 1 MiB
aggregate filesize  : 2 GiB

Results:

access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter
------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----
Commencing write performance test: Mon Jun  1 19:21:30 2020
write     24.38      24.39      83.98       1024.00    1024.00    0.004828   83.98      0.000897   83.99      0
Max Write: 24.38 MiB/sec (25.57 MB/sec)

Summary of all tests:
Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev   Max(OPs)   Min(OPs)  Mean(OPs)     StdDev    Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt   blksiz    xsize aggs(MiB)   API RefNum
write          24.38      24.38      24.38       0.00      24.38      24.38      24.38       0.00   83.99048         NA            NA     0      1   1    1   0     0        1         0    0   2048  1048576  1048576    2048.0 POSIX      0
Finished            : Mon Jun  1 19:22:54 2020
----
. Record the results somewhere (e.g. your favorite text editor).
. How long did it take (total seconds)?
. What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate one 2 GiB file using two threads on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 2 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 1024 -g -v -w -i 1 -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?
+
. *_Run_* the following IOR command to generate one 2 GiB file using two threads on an EFS file system.
+
[source,bash]
----
sudo bash -c 'echo 3 > /proc/sys/vm/drop_caches'
mpirun --npernode 4 --oversubscribe ior --posix.odirect -t 1m -b 1m -s 512 -g -v -w -i 1 -k -D 0 -o /efs/ior/ior.bin

----
+
* Record the results somewhere (e.g. your favorite text editor).
* How long did it take (total seconds)?
* What was the bandwidth or throughput (MB/s)?


























. *_Run_* this command to generate 2GB of data on the EFS file system using a 1 MB block size and issuing a sync once at the end to ensure everything is written to disk.
+
[source,bash]
----
time dd if=/dev/zero of=/efs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N) bs=1M count=2048 status=progress conv=fsync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 11.516s


. *_Run_* this command to generate 2GB of data on the EBS volume using a 16 MB block size and issuing a sync once at the end to ensure everything is written to disk.
+
[source,bash]
----
time dd if=/dev/zero of=/ebs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N) bs=16M count=128 status=progress conv=fsync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 17.757s


. *_Run_* this command to generate 2GB of data on the EFS file system using a 16 MB block size and issuing a sync once at the end to ensure everything is written to disk.
+
[source,bash]
----
time dd if=/dev/zero of=/efs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N) bs=16M count=128 status=progress conv=fsync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 11.611s


. *_Run_* this command to generate 2GB of data on the EBS volume using a 1 MB block size and issuing a sync after each block to ensure each block is written to disk.
+
[source,bash]
----
time dd if=/dev/zero of=/ebs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N) bs=1M count=2048 status=progress oflag=sync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 15.002s


. *_Run_* this command to generate 2GB of data on the EFS file system using a 1 MB block size and issuing a sync once at the end to ensure everything is written to disk.
+
[source,bash]
----
time dd if=/dev/zero of=/efs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N) bs=1M count=2048 status=progress oflag=sync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 1m26.699s
. Why did this take so long?
* A sync operation that persists data and metadata to disk is issued after everyone 1MB block, so there are 2048 sync operations issues. The distributed data storage design of Amazon EFS introduces slightly higher latencies per file system operation, and because this dd command is a serial operation, it needs to wait for each 1MB block to persist to disk before starting to write the next 1MB block. This type of operation magnifies the higher latencies of Amazon EFS.


. *_Run_* this command to generate 2GB of data on the EBS volume using a 16 MB block size and issuing a sync after each block to ensure each block is written to disk.
+
[source,bash]
----
time dd if=/dev/zero of=/ebs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N) bs=16M count=128 status=progress oflag=sync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 15.002s


. *_Run_* this command to generate 2GB of data on the EFS file system using a 16 MB block size and issuing a sync once at the end to ensure everything is written to disk.
+
[source,bash]
----
time dd if=/dev/zero of=/efs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N) bs=16M count=128 status=progress oflag=sync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 30.574s
. Is there a significant duration difference between the commands writing to the EBS volume?
. Why not?
* The latency per file system operation is very low so the number of sync operations doesn't make a significant difference.
. Why is there such a duration variance between the commands writing to the EFS file system?
. The distributed data storage design of Amazon EFS introduces slightly higher latencies per file system operation, and because this dd command is a serial operation, it needs to wait for each block to persist to disk before starting to write the next block. Less sync operations increases achievable throughput.


=== Different levels of parallelism

IMPORTANT: Read through all steps below and watch the quick video before continuing.

image::throughput-dd.gif[align="left", width=600]

. *_Run_* this command to generate 2GB of data on the EBS volume using 4 threads in parallel and a 1 MB block size, issuing a sync after each block to ensure everything is written to disk.
+
[source,bash]
----
time seq 1 4 | parallel --will-cite -j 4 dd if=/dev/zero of=/ebs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N)-{} bs=1M count=512 oflag=sync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 15.083s

. *_Run_* this command to generate 2GB of data on the EFS file system using 4 threads in parallel and a 1 MB block size, issuing a sync after each block to ensure everything is written to disk.
+
[source,bash]
----
time seq 1 4 | parallel --will-cite -j 4 dd if=/dev/zero of=/efs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N)-{} bs=1M count=512 oflag=sync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 0m23.292s
. Compare this to the results above when you wrote to the EFS file system using 1 thread and a 1 MB block size, issuing a sync after each block. Is there a big difference? Why?


. *_Run_* this command to generate 2GB of data on the EBS volume using 16 threads in parallel and a 1 MB block size, issuing a sync after each block to ensure everything is written to disk.
+
[source,bash]
----
time seq 1 16 | parallel --will-cite -j 16 dd if=/dev/zero of=/ebs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N)-{} bs=1M count=128 oflag=sync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 15.093s

. *_Run_* this command to generate 2GB of data on the EFS file system using 16 threads in parallel and a 1 MB block size, issuing a sync after each block to ensure everything is written to disk.
+
[source,bash]
----
time seq 1 16 | parallel --will-cite -j 16 dd if=/dev/zero of=/efs/dd/2G-dd-$(date +%Y%m%d%H%M%S.%3N)-{} bs=1M count=128 oflag=sync
----
+
. Record these results somewhere (e.g. your favorite text editor).
. How long did it take? - 0m10.581s
. Compare this to the results above when you wrote to the EFS file system using 1 thread and a 1 MB block size, issuing a sync after each block. Is there a big difference? Why?
. Review the results of all the EBS tests. Was there a significant difference between any of them?
. Review the results of all the EFS tests. Why was there a significant difference between them?
. Where you able to achieve higher overall throughput writting to an EFS file system than a local EBS volume?

* The following table and graphs show the sample results of these tests. Look how increasing the size of the IO (reducing sync operations) and increasing the number of threads (increasing parallelism) impacts the throughput and duration.

+

|==============================================================================================
| Storage | Threads | Data size (MB) | Block size (MB) | Duration (seconds) | Throughput (MB/s)
| EBS     | 1       | 2048           | 1               | 15.002             | 136.5
| EFS     | 1       | 2048           | 1               | 86.699             | 23.6
| EBS     | 1       | 2048           | 16              | 15.002             | 136.5
| EFS     | 1       | 2048           | 16              | 30.574             | 67.0
| EBS     | 4       | 2048           | 1               | 15.083             | 135.8
| EFS     | 4       | 2048           | 1               | 23.292             | 87.9
| EBS     | 16      | 2048           | 1               | 15.093             | 135.7
| EFS     | 16      | 2048           | 1               | 10.581             | 193.6
|==============================================================================================

--
[.left]
.IOPS
image::throughput-dd-throughput-graph.png[450, scaledwidth="75%"]
[.left]
.Duration
image::throughput-dd-duration-graph.png[450, scaledwidth="75%"]
--


== Next section

Click the link below to go to the next section.

image::transfer-tools.png[link=../09-transfer-tools, align="left",width=420]

